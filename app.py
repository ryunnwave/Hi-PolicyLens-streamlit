# -*- coding: utf-8 -*-
"""
RegWatch â€“ ì¹´ë“œí˜• ê°•í™”ìš”ì•½ (ì •ì±…/ë³¸ë¬¸ í•„í„° ì—†ìŒ / ì¬ê·€ ì˜¤ë¥˜ ìˆ˜ì •íŒ)
- ëª¨ë“  í•­ëª© ìˆ˜ì§‘ â†’ ìƒì„¸ í˜ì´ì§€/ë˜ëŠ” PDF ë³¸ë¬¸ì„ ì½ì–´ êµ¬ì¡°ì  ìš”ì•½ ìƒì„±
- í•˜ì´ë¼ì´íŠ¸(ì£¼ìš” ë³€ê²½ì‚¬í•­), ì‹œí–‰/ë°œíš¨ ì¶”ì •, ì˜í–¥ ë²”ìœ„, íƒœê·¸ ìë™í™”
- ì†ŒìŠ¤: EEA Analysis, CBP RSS, BMUV RSS, MOTIE ë£¨íŠ¸, Bundesregierung Aktuelles,
        CBP Bulletin(í˜ì´ì§€ ë‚´ PDF), U.S. DOE Newsroom
"""

import re, io, hashlib
from datetime import datetime, timezone, timedelta
from typing import List, Dict

import requests, feedparser, pandas as pd
from bs4 import BeautifulSoup
from dateutil import parser as dtparse
import streamlit as st

# PDF
try:
    from pypdf import PdfReader
    HAVE_PYPDF = True
except Exception:
    HAVE_PYPDF = False
try:
    from pdfminer.high_level import extract_text as pdfminer_extract_text
    HAVE_PDFMINER = True
except Exception:
    HAVE_PDFMINER = False

# ----------------------- UI ê¸°ë³¸ -----------------------
st.set_page_config(page_title="RegWatch â€“ ê°•í™”ìš”ì•½", layout="wide")
BRAND = "#0f2e69"; ACCENT = "#dc8d32"; BG = "#0f2e69"

st.markdown(f"""
<style>
@import url('https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.8/dist/web/static/pretendard.css');
:root {{ --brand:{BRAND}; --accent:{ACCENT}; --muted:#8aa0c6; --ink:#0b1a3a; }}
html, body, [class*="css"] {{ font-family:'Pretendard', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif; }}
.main {{ background:#f6f8fb; }}
.hero {{
  background: linear-gradient(135deg, {BG} 0%, #1a4b8c 100%);
  color:#fff; padding:18px 22px; border-radius:14px; margin:6px 0 14px;
  box-shadow:0 6px 20px rgba(0,0,0,.12)
}}
.hero .title {{ font-size:24px; font-weight:900; margin:0 }}
.hero .subtitle {{ opacity:.92; margin-top:6px }}
.badge {{ display:inline-block; padding:3px 9px; border-radius:999px; font-size:11px; font-weight:800; background:#fff; color:var(--brand); margin-left:8px }}
.kpi {{ background:#fff; border:1px solid #e2e8f0; border-radius:12px; padding:14px; text-align:center }}
.kpi .num {{ font-weight:900; font-size:22px; color:var(--brand) }}
.kpi .lab {{ color:#64748b; font-size:12px; margin-top:6px }}
hr.sep {{ border:none; border-top:1px solid #e2e8f0; margin:16px 0 }}

.bigcard {{
  position:relative;
  background:#0f2e69; color:#e6eefc;
  border-radius:16px; padding:18px 18px 14px; margin:14px 0;
  box-shadow:0 10px 28px rgba(15,46,105,.18); border:1px solid rgba(255,255,255,.06);
}}
.bigcard h3 {{ color:#ffffff; margin:0 0 10px; font-size:20px; line-height:1.35; letter-spacing:-.2px }}
.chips {{ display:flex; gap:8px; flex-wrap:wrap; margin-bottom:10px }}
.chip {{ font-size:11px; font-weight:800; padding:4px 10px; border-radius:999px; background:#0b1a3a; color:#dbe7ff; border:1px solid rgba(255,255,255,.1) }}
.chip.new {{ background:#ffedd5; color:#b45309 }}
.meta {{ display:grid; grid-template-columns:1fr 1fr; gap:10px; margin:10px 0 6px }}
.meta .box {{ background:rgba(255,255,255,.06); border:1px solid rgba(255,255,255,.08); border-radius:10px; padding:8px 10px; font-size:12px }}
.section {{ background:#0b1a3a; border:1px solid rgba(255,255,255,.08); border-radius:12px; padding:10px 12px; margin-top:10px }}
.section h4 {{ margin:0 0 6px; font-size:13px; color:#cfe2ff }}
.tags {{ margin-top:10px }}
.tag {{ display:inline-block; font-size:11px; padding:3px 10px; border-radius:999px; background:#08142c; color:#bcd2ff; margin-right:6px; border:1px solid rgba(255,255,255,.06) }}
.cta {{ margin-top:10px }}
a.btn {{
  display:inline-block; background:#fff; color:#0f2e69; font-weight:800; font-size:13px;
  padding:8px 14px; border-radius:10px; text-decoration:none; border:1px solid #e2e8f0;
}}
small.mono {{ font-family:ui-monospace,Menlo,Consolas,"Courier New",monospace; color:#9fb5db }}
</style>
""", unsafe_allow_html=True)

st.markdown(f"""
<div class="hero">
  <div class="title">RegWatch <span class="badge">ê°•í™”ìš”ì•½</span></div>
  <div class="subtitle">í•„í„° ì—†ì´ ìˆ˜ì§‘ â†’ ìƒì„¸ ë³¸ë¬¸Â·PDFë¥¼ ì½ì–´ ì¹´ë“œí˜• ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.</div>
</div>
""", unsafe_allow_html=True)

# ----------------------- ì†ŒìŠ¤ ì •ì˜ -----------------------
USER_AGENT = "Mozilla/5.0 (compatible; RegWatch/2.1; +https://streamlit.io)"
HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ko,en;q=0.8"}
TIMEOUT = 25
MAX_PER_SOURCE = 60

SOURCES = [
    {"id":"eea",   "name":"EEA â€“ Analysis", "type":"html", "url":"https://www.eea.europa.eu/en/analysis"},
    {"id":"cbp",   "name":"CBP â€“ RSS", "type":"rss-multi",
     "urls":["https://www.cbp.gov/rss/trade","https://www.cbp.gov/rss/press-releases"]},
    {"id":"bmuv",  "name":"BMUV â€“ Meldungen RSS", "type":"rss", "url":"https://www.bundesumweltministerium.de/meldungen.rss"},
    {"id":"motie", "name":"MOTIE (ì‚°ì—…ë¶€) ë£¨íŠ¸", "type":"html", "url":"https://www.motie.go.kr/"},
    {"id":"bund",  "name":"Bundesregierung â€“ Aktuelles", "type":"html", "url":"https://www.bundesregierung.de/breg-de/aktuelles"},
    {"id":"cbp_bull","name":"CBP â€“ Bulletin/Decisions (PDF)", "type":"html", "url":"https://www.cbp.gov/trade/rulings/bulletin-decisions"},
    {"id":"doe",   "name":"U.S. DOE â€“ Newsroom", "type":"html", "url":"https://www.energy.gov/newsroom"},
]
COUNTRY = {"cbp":"ë¯¸êµ­","cbp_bull":"ë¯¸êµ­","bmuv":"ë…ì¼","motie":"ëŒ€í•œë¯¼êµ­","eea":"EU","bund":"ë…ì¼","doe":"ë¯¸êµ­"}

# ----------------------- ë¡œê·¸ -----------------------
if "logs" not in st.session_state: st.session_state.logs=[]
def log(msg): st.session_state.logs.append(f"[{datetime.now().strftime('%H:%M:%S')}] {msg}")
def clear_logs(): st.session_state.logs=[]

# ----------------------- ìœ í‹¸ -----------------------
def md5_hex(s:str)->str: return hashlib.md5(s.encode("utf-8")).hexdigest()
def clean(s:str)->str: return re.sub(r"\s+"," ", (s or "")).strip()
def to_iso(s:str)->str:
    if not s: return datetime.now(timezone.utc).isoformat()
    try:
        d=dtparse.parse(s); d=d if d.tzinfo else d.replace(tzinfo=timezone.utc)
        return d.astimezone(timezone.utc).isoformat()
    except Exception:
        return datetime.now(timezone.utc).isoformat()
def title_from_url(url:str)->str:
    from urllib.parse import urlparse, unquote
    p=urlparse(url); segs=[unquote(x) for x in p.path.split("/") if x]
    if not segs: return p.netloc
    return " / ".join(segs[-3:]).replace("-"," ").title()

def extract_keywords(text:str, topn=6):
    stop=set(["the","and","for","with","from","that","this","are","was","were","will","have","has","been","on","of","in","to","a","an","by",
              "ë°","ê³¼","ì—","ì˜","ìœ¼ë¡œ","ëŒ€í•œ","ê´€ë ¨"])
    words=re.sub(r"[^a-z0-9ê°€-í£ ]"," ", (text or "").lower()).split()
    freq={}
    for w in words:
        if len(w)<=2 or w in stop: continue
        freq[w]=freq.get(w,0)+1
    return [w for w,_ in sorted(freq.items(), key=lambda x:x[1], reverse=True)[:topn]]

def guess_category(title:str)->str:
    t=(title or "").lower()
    if re.search(r"(reach|clp|pfas|biocide|chemical|substance|restriction|authorisation)", t): return "í™”í•™ë¬¼ì§ˆê·œì œ"
    if re.search(r"(tariff|duty|quota|rate|import|export|cbp|customs|ruling|bulletin)", t): return "ë¬´ì—­ì •ì±…"
    if re.search(r"(environment|climate|emission|umwelt|í™˜ê²½|energy|ì „ë ¥|ì—ë„ˆì§€)", t): return "í™˜ê²½ê·œì œ"
    if re.search(r"(policy|industry|manufactur|ì‚°ì—…|ì „ëµ|íˆ¬ì)", t): return "ì‚°ì—…ì •ì±…"
    return "ì‚°ì—…ì •ì±…"

# ----------------------- HTTP -----------------------
@st.cache_data(ttl=1500, show_spinner=False)
def http_get(url:str)->str:
    r=requests.get(url, headers=HEADERS, timeout=TIMEOUT); r.raise_for_status(); return r.text
def http_get_bytes(url:str)->bytes:
    r=requests.get(url, headers=HEADERS, timeout=TIMEOUT); r.raise_for_status(); return r.content
def fetch_with_fallback(url:str)->str:
    try:
        return http_get(url)
    except Exception as ex:
        log(f"ì§ì ‘ìš”ì²­ ì‹¤íŒ¨ â†’ í”„ë¡ì‹œ ì‹œë„: {url} ({type(ex).__name__})")
        base=re.sub(r"^https?://","", url)
        for prefix in ("https://r.jina.ai/http://","https://r.jina.ai/https://"):
            try:
                txt=http_get(prefix+base)
                if txt and len(txt)>100: return txt
            except Exception: pass
        raise
def fetch_bytes_with_fallback(url:str)->bytes:
    try:
        return http_get_bytes(url)
    except Exception:
        txt=fetch_with_fallback(url)
        return txt.encode("utf-8", errors="ignore")

# ----------------------- PDF -----------------------
def is_pdf_url(url:str)->bool:
    return url.split("?")[0].lower().endswith(".pdf")

def pdf_bytes_to_text(data:bytes, max_pages=14, max_chars=60000)->str:
    if HAVE_PYPDF:
        try:
            reader=PdfReader(io.BytesIO(data))
            parts=[]
            for p in reader.pages[:max_pages]:
                try: parts.append(p.extract_text() or "")
                except Exception: break
            txt=clean(" ".join(parts))
            if len(txt)>80: return txt[:max_chars]
        except Exception: pass
    if HAVE_PDFMINER:
        try:
            txt=pdfminer_extract_text(io.BytesIO(data))
            return clean(txt)[:max_chars]
        except Exception: pass
    return ""

def fetch_pdf_text(url:str)->str:
    data=fetch_bytes_with_fallback(url)
    return pdf_bytes_to_text(data)

def find_pdf_in_html(html:str, base_url:str)->str|None:
    soup=BeautifulSoup(html,"html.parser")
    for a in soup.find_all("a", href=True):
        href=a["href"].strip()
        if not href: continue
        if href.lower().endswith(".pdf"):
            from urllib.parse import urljoin
            return href if href.startswith("http") else urljoin(base_url, href)
    return None

# ----------------------- ë³¸ë¬¸ ì¶”ì¶œ & ê°•í™” ìš”ì•½ -----------------------
def extract_page_text(html:str)->str:
    soup=BeautifulSoup(html,"html.parser")
    meta=soup.find("meta", attrs={"name":"description"}) or soup.find("meta", attrs={"property":"og:description"})
    if meta and meta.get("content"): return clean(meta["content"])
    for sel in ["article","main","[role=main]",".content",".article",".post",".richtext",".usa-prose",".body-content"]:
        for tag in soup.select(sel):
            txt=tag.get_text(" ", strip=True)
            if txt and len(txt)>160: return clean(txt)
    ps=soup.find_all("p")
    return clean(" ".join(p.get_text(' ', strip=True) for p in ps[:6]))

DATE_PAT = r"(20\d{2}[-./ë…„ ]\s?(?:0?[1-9]|1[0-2])[-./ì›” ]\s?(?:0?[1-9]|[12]\d|3[01])(?:ì¼)?)"
HL_WORDS = r"(must|shall|required|require|ban|prohibit|effective|applicab|enforce|submit|report|duty|tariff|quota|amend|revise|extend|deadline|from|until|by|notice|guidance|draft|consultation|ê°œì •|ì‹œí–‰|ë°œíš¨|ì˜ë¬´|ì œì¶œ|ë³´ê³ |ê¸ˆì§€|ì—°ì¥|ì˜ê²¬ìˆ˜ë ´|ì´ˆì•ˆ)"

def split_sents(text:str)->List[str]:
    t=clean(text)
    try: s=re.split(r'(?:(?<=\.)|(?<=!)|(?<=\?)|(?<=ë‹¤\.)|(?<=ìš”\.))\s+', t)
    except re.error: s=re.split(r'[.!?]\s+', t)
    return [x.strip() for x in s if x.strip()]

def pick_highlights(text: str, topn=5) -> List[str]:
    sents = split_sents(text)
    hits = [s for s in sents if re.search(HL_WORDS, s, re.I)]
    if not hits:
        hits = sents[:topn]
    seen, out = set(), []
    for h in hits:
        key = h.lower()
        if key in seen:
            continue
        seen.add(key)
        out.append(h[:220])
        if len(out) >= topn:
            break
    return out

def guess_effective_date(text:str)->str:
    m=re.search(r"(effective|applicable|enforce(?:ment)?|ì‹œí–‰|ë°œíš¨)[^\.]{0,40}?"+DATE_PAT, text, re.I)
    if not m:
        m=re.search(DATE_PAT, text)
    if not m: return ""
    try: return to_iso(m.group(0).replace("ë…„","-").replace("ì›”","-").replace("ì¼",""))
    except Exception: return ""

def guess_scope(text:str)->str:
    t=text.lower()
    scope=[]
    if re.search(r"importer|exporter|customs|trade|supply|supply chain|import|export|duty|tariff", t): scope.append("ìˆ˜ì…Â·ë¬´ì—­ì£¼ì²´")
    if re.search(r"manufacturer|producer|chemical|factory|industrial", t): scope.append("ì œì¡°ì—…")
    if re.search(r"energy|fuel|electric|power|renewable|grid", t): scope.append("ì—ë„ˆì§€")
    if re.search(r"retail|consumer|product", t): scope.append("ì†Œë¹„ì¬")
    if re.search(r"financial|bank|insurance", t): scope.append("ê¸ˆìœµ")
    return " Â· ".join(scope) or "ì¼ë°˜ ê¸°ì—…/ê¸°ê´€"

def strengthen_summary(title: str, body: str, power: int = 4) -> Dict[str, str]:
    """ìš”ì•½ ê°•ë„(power)ë¥¼ ì¸ìë¡œ ë°›ì•„ ì¬ê·€ ì—†ì´ ê¸¸ì´/í•˜ì´ë¼ì´íŠ¸ ê°œìˆ˜ë§Œ ì¡°ì ˆ"""
    sents = split_sents(body)
    abs_sent_n = min(2 + power, 6)                    # 3~6ë¬¸ì¥
    abstract = " ".join(sents[:abs_sent_n])[:480] if sents else body[:480]

    hl_topn = min(2 + power, 8)                       # 3~8ê°œ
    bullets = pick_highlights(body, topn=hl_topn)
    if not bullets:
        bullets = sents[:min(3, len(sents))]

    eff = guess_effective_date(body)
    scope = guess_scope(body)
    tags = extract_keywords(title + " " + body, topn=6)
    return {"abstract": abstract, "bullets": bullets, "effective": eff, "scope": scope, "tags": tags}

def scrape_detail(url:str)->Dict[str,str]:
    out={"body":"","date":""}
    try:
        if is_pdf_url(url):
            out["body"]=fetch_pdf_text(url); return out
        html=fetch_with_fallback(url)
        body=extract_page_text(html); out["body"]=body
        soup=BeautifulSoup(html,"html.parser")
        dt = soup.find("time") or soup.find("meta", attrs={"property":"article:published_time"}) or soup.find("meta", attrs={"name":"date"})
        if dt:
            out["date"] = dt.get("datetime") or dt.get("content") or dt.get_text(strip=True) or ""
        if len(out["body"])<120:
            pdf=find_pdf_in_html(html, url)
            if pdf: out["body"]=fetch_pdf_text(pdf)
    except Exception as ex:
        log(f"ìƒì„¸ ì¶”ì¶œ ì‹¤íŒ¨: {type(ex).__name__}")
    return out

# ----------------------- ìˆ˜ì§‘ê¸° -----------------------
def normalize(source_id, name, title, url, date_iso, summary)->Dict:
    if not url: return {}
    return {
        "id": md5_hex(url), "sourceId": source_id, "sourceName": name,
        "title": clean(title or url), "url": url, "dateIso": to_iso(date_iso or ""),
        "category": guess_category(title or url), "country": COUNTRY.get(source_id,""),
        "summary": clean(summary or "")
    }

def fetch_rss(feed_url, sid, name):
    items=[]
    try:
        raw=fetch_bytes_with_fallback(feed_url)
        d=feedparser.parse(raw)
        for e in d.entries[:MAX_PER_SOURCE]:
            items.append(normalize(sid,name,
                getattr(e,"title",""), getattr(e,"link",""),
                getattr(e,"published","") or getattr(e,"updated","") or "",
                getattr(e,"summary","") or getattr(e,"description","") or ""))
        log(f"RSS OK [{name}] {len(items)}ê±´")
    except Exception as ex:
        log(f"RSS FAIL [{name}] {type(ex).__name__}: {ex}")
    return items

def fetch_rss_multi(urls, sid, name):
    out=[]
    for u in urls: out += fetch_rss(u, sid, name)
    uniq={}; [uniq.setdefault(i["url"], i) for i in out]
    return list(uniq.values())[:MAX_PER_SOURCE]

def fetch_links_by_contains(url, must_contains:str, sid, name, base=None):
    items=[]
    try:
        html=fetch_with_fallback(url)
        soup=BeautifulSoup(html,"html.parser")
        add=0
        for a in soup.find_all("a", href=True):
            href=a["href"].strip(); text=a.get_text(" ", strip=True)
            if not href or not text: continue
            if must_contains and must_contains not in href: continue
            if not href.startswith("http"):
                from urllib.parse import urljoin
                href=urljoin(base or url, href)
            items.append(normalize(sid,name,text,href,"",text)); add+=1
            if len(items)>=MAX_PER_SOURCE: break
        log(f"{name} ë§í¬ {add}ê±´")
    except Exception as ex:
        log(f"{name} FAIL {type(ex).__name__}: {ex}")
    uniq={}; [uniq.setdefault(i["url"], i) for i in items]
    return list(uniq.values())[:MAX_PER_SOURCE]

def fetch_motie_root():
    return fetch_links_by_contains("https://www.motie.go.kr/", "", "motie", "MOTIE", base="https://www.motie.go.kr/")

def fetch_all(selected)->List[Dict]:
    out=[]
    for s in SOURCES:
        if s["id"] not in selected: continue
        if s["id"]=="bmuv":
            out += fetch_rss(s["url"], s["id"], s["name"])
        elif s["id"]=="cbp":
            out += fetch_rss_multi(s["urls"], s["id"], s["name"])
        elif s["id"]=="eea":
            out += fetch_links_by_contains(s["url"], "/en/analysis", s["id"], s["name"], base="https://www.eea.europa.eu")
        elif s["id"]=="bund":
            out += fetch_links_by_contains(s["url"], "/breg-de/aktuelles", s["id"], s["name"], base="https://www.bundesregierung.de")
        elif s["id"]=="cbp_bull":
            out += fetch_links_by_contains(s["url"], ".pdf", s["id"], s["name"], base="https://www.cbp.gov")
        elif s["id"]=="doe":
            out += fetch_links_by_contains(s["url"], "/newsroom", s["id"], s["name"], base="https://www.energy.gov")
        elif s["id"]=="motie":
            out += fetch_motie_root()
    uniq={}; [uniq.setdefault(i["url"], i) for i in out]
    log(f"ì´ ìˆ˜ì§‘ {len(uniq)}ê±´")
    return list(uniq.values())

# ----------------------- ì»¨íŠ¸ë¡¤ -----------------------
c1,c2,c3,c4 = st.columns([2,1.2,1.2,1.6])
with c1:
    selected=st.multiselect("ìˆ˜ì§‘ ëŒ€ìƒ", [s["id"] for s in SOURCES],
        default=[s["id"] for s in SOURCES],
        format_func=lambda sid: next(s["name"] for s in SOURCES if s["id"]==sid))
with c2:
    since_days=st.slider("ìµœê·¼ Nì¼", 3, 90, 14)
with c3:
    summary_power=st.select_slider("ìš”ì•½ ê°•ë„", options=[1,2,3,4,5], value=4,
        help="ê°’ì´ í´ìˆ˜ë¡ ìš”ì•½ì„ ê¸¸ê³ /í•˜ì´ë¼ì´íŠ¸ ë” ë§ì´ ìƒì„±")
with c4:
    user_terms=st.text_input("ì¶”ê°€ í‚¤ì›Œë“œ(ì„ íƒ, ì‰¼í‘œêµ¬ë¶„)", value="")  # ê²€ìƒ‰ì—ë§Œ ì‚¬ìš©

a,b,d = st.columns([1,1,2])
with a:
    do=st.button("ì—…ë°ì´íŠ¸ ì‹¤í–‰", use_container_width=True)
with b:
    if st.button("ìºì‹œ ì´ˆê¸°í™”", use_container_width=True):
        st.cache_data.clear(); clear_logs(); st.success("HTTP ìºì‹œ ë¹„ì›€")
with d:
    show_debug=st.toggle("ë””ë²„ê·¸", value=False)

st.markdown("<hr class='sep'/>", unsafe_allow_html=True)

# ----------------------- ì‹¤í–‰ -----------------------
if do or "raw" not in st.session_state:
    clear_logs()
    with st.spinner("ìˆ˜ì§‘ ì¤‘..."):
        st.session_state.raw = fetch_all(selected or [s["id"] for s in SOURCES])

raw = st.session_state.get("raw", [])

# ê¸°ê°„ í•„í„°
cut = datetime.now(timezone.utc) - timedelta(days=since_days)
def in_range(iso):
    try: return dtparse.parse(iso) >= cut
    except: return True
items = [d for d in raw if in_range(d["dateIso"])]

# ìƒì„¸ Â· ê°•í™”ìš”ì•½
processed=[]
with st.spinner("ìƒì„¸ ë³¸ë¬¸/ PDF ìš”ì•½ ìƒì„± ì¤‘..."):
    for it in items:
        detail = scrape_detail(it["url"])
        body = detail.get("body","")
        if not body or len(body) < 60:
            body = it.get("summary") or title_from_url(it["url"])
        if detail.get("date"): it["dateIso"] = to_iso(detail["date"])
        y = strengthen_summary(it["title"], body, power=summary_power)
        it.update({
            "abs": y["abstract"],
            "bullets": y["bullets"],
            "effective": y["effective"] or "-",
            "scope": y["scope"],
            "tags": y["tags"],
        })
        processed.append(it)

# KPI
k1,k2,k3,k4 = st.columns(4)
k1.markdown(f"<div class='kpi'><div class='num'>{len(raw)}</div><div class='lab'>ìˆ˜ì§‘(ì›ë³¸)</div></div>", unsafe_allow_html=True)
k2.markdown(f"<div class='kpi'><div class='num'>{len(items)}</div><div class='lab'>ìµœê·¼ {since_days}ì¼</div></div>", unsafe_allow_html=True)
k3.markdown(f"<div class='kpi'><div class='num'>{len(processed)}</div><div class='lab'>ìš”ì•½ ìƒì„± ì„±ê³µ</div></div>", unsafe_allow_html=True)
k4.markdown(f"<div class='kpi'><div class='num'>{datetime.now().strftime('%Y-%m-%d %H:%M')}</div><div class='lab'>ìµœì¢… ì—…ë°ì´íŠ¸</div></div>", unsafe_allow_html=True)

# ê²€ìƒ‰/ì¹´í…Œê³ ë¦¬
st.markdown("<hr class='sep'/>", unsafe_allow_html=True)
f1,f2 = st.columns([2,1])
with f1: q=st.text_input("ê²€ìƒ‰(ì œëª©/ìš”ì•½/ê¸°ê´€/êµ­ê°€)", value=user_terms)
with f2: cat=st.selectbox("ì¹´í…Œê³ ë¦¬", ["ì „ì²´","í™”í•™ë¬¼ì§ˆê·œì œ","ë¬´ì—­ì •ì±…","ì‚°ì—…ì •ì±…","í™˜ê²½ê·œì œ"])

data=processed
if q:
    ql=q.lower()
    data=[d for d in data if ql in (d["title"]+" "+d.get("abs","")+" "+d["sourceName"]+" "+d.get("country","")).lower()]
if cat!="ì „ì²´":
    data=[d for d in data if d["category"]==cat]

# í‘œ + ë‹¤ìš´ë¡œë“œ
df = pd.DataFrame([{
    "date":d["dateIso"], "title":d["title"], "agency":d["sourceName"], "country":d.get("country",""),
    "category":d["category"], "effective":d.get("effective",""), "url":d["url"]
} for d in data])
try:
    df["date_dt"]=pd.to_datetime(df["date"], errors="coerce")
    df=df.sort_values("date_dt", ascending=False).drop(columns=["date_dt"])
except: pass

st.subheader(f"ëª©ë¡ Â· ì´ {len(df)}ê±´")
st.dataframe(df, use_container_width=True, hide_index=True)

# ì¹´ë“œ
def days_new(iso:str, days=7):
    try: d=dtparse.parse(iso); d=d if d.tzinfo else d.replace(tzinfo=timezone.utc)
    except Exception: return False
    return (datetime.now(timezone.utc)-d)<=timedelta(days=days)

st.markdown("<hr class='sep'/>", unsafe_allow_html=True)
st.subheader("ì¹´ë“œ ë³´ê¸°")

for d in data:
    is_new = days_new(d["dateIso"], 7)
    chips = f"<span class='chip'>{d['category']}</span>"
    if is_new: chips += "<span class='chip new'>NEW</span>"
    if d.get("country"): chips += f"<span class='chip'>{d['country']}</span>"

    bullets = d.get("bullets") or []
    bl_html = "".join([f"<li>{re.sub(r'<.*?>','',b)}</li>" for b in bullets])

    tags_html = " ".join([f"<span class='tag'>{t}</span>" for t in (d.get('tags') or [])])

    st.markdown("<div class='bigcard'>", unsafe_allow_html=True)
    st.markdown(f"<div class='chips'>{chips}</div>", unsafe_allow_html=True)
    st.markdown(f"<h3>{d['title']}</h3>", unsafe_allow_html=True)
    st.write(d.get("abs",""))

    st.markdown("<div class='section'><h4>âš¡ ì£¼ìš” ë³€ê²½ì‚¬í•­</h4><ul>"+bl_html+"</ul></div>", unsafe_allow_html=True)

    eff = d.get("effective","-")
    st.markdown(
        f"<div class='meta'>"
        f"  <div class='box'><b>ë‹´ë‹¹ ê¸°ê´€</b><br>{d['sourceName']}</div>"
        f"  <div class='box'><b>ì‹œí–‰/ë°œíš¨</b><br>{eff}</div>"
        f"</div>",
        unsafe_allow_html=True
    )
    st.markdown(f"<div class='section'><h4>ğŸ“Œ ì˜í–¥ ë²”ìœ„</h4>{d.get('scope','')}</div>", unsafe_allow_html=True)

    if tags_html:
        st.markdown(f"<div class='tags'>{tags_html}</div>", unsafe_allow_html=True)

    st.markdown(f"<div class='cta'><a class='btn' href='{d['url']}' target='_blank'>ì›ë¬¸ ë³´ê¸°</a> &nbsp; <small class='mono'>{d['dateIso']}</small></div>", unsafe_allow_html=True)
    st.markdown("</div>", unsafe_allow_html=True)

# ë””ë²„ê·¸
if show_debug:
    with st.expander("ë””ë²„ê·¸: ë¡œê·¸/ì›ì‹œë°ì´í„°"):
        st.json(st.session_state.get("raw", []), expanded=False)
        if st.session_state.logs:
            st.markdown("<small class='mono'>"+"<br/>".join(st.session_state.logs)+"</small>", unsafe_allow_html=True)

