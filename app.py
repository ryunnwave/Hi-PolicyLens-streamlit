# -*- coding: utf-8 -*-
"""
RegWatch (Streamlit)
- ëŒ€ìƒ: ECHA(legislation/news), CBP(trade/press RSS), MOTIE(êµ­ë¬¸ ì‚¬ì´íŠ¸), BMUV(RSS)
- ê¸°ëŠ¥: ìˆ˜ì§‘ â†’ ìš”ì•½(Potens API ì˜µì…˜) â†’ í‘œ/ì¹´ë“œ/ë³´ê³ ì„œ(Markdown) + ë‹¤ìš´ë¡œë“œ
"""

import os, re, hashlib, io, csv
from datetime import datetime, timezone, timedelta
from typing import List, Dict

import requests, feedparser, pandas as pd
from bs4 import BeautifulSoup
from dateutil import parser as dtparse
import streamlit as st

# ---------------------------
# UI ê¸°ë³¸ ì„¸íŒ…
# ---------------------------
st.set_page_config(page_title="RegWatch â€“ ê¸€ë¡œë²Œ ê·œì œ ëª¨ë‹ˆí„°ë§", layout="wide")
BRAND = "#0f2e69"; ACCENT = "#dc8d32"
st.markdown(f"""
<style>
@import url('https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.8/dist/web/static/pretendard.css');
html, body, [class*="css"] {{ font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif; }}
.big-header {{ background: linear-gradient(135deg, {BRAND} 0%, #1a4b8c 100%); color:#fff; padding:18px 22px; border-radius:10px; margin-bottom:12px; }}
.brand-title {{ font-weight:800; font-size:22px; margin-right:10px; }}
.card {{ border:1px solid #e2e8f0; border-left:5px solid transparent; border-radius:12px; padding:16px; margin:12px 0; background:#fff; }}
.card.new {{ border-left-color:{ACCENT}; }}
.card h4 {{ color:{BRAND}; margin:0 0 8px 0; font-size:18px; }}
.badge {{ display:inline-block; padding:3px 8px; border-radius:999px; font-size:11px; font-weight:700; margin-right:6px; }}
.badge.kor {{ background:#f1f5f9; color:#475569; }}
.badge.cat-chem {{ background:#e0f2fe; color:#0277bd; }}
.badge.cat-trade {{ background:#f3e5f5; color:#7b1fa2; }}
.badge.cat-ind {{ background:#e8f5e8; color:#2e7d32; }}
.badge.cat-env {{ background:#fff3e0; color:#f57c00; }}
.badge.status-ann {{ background:#dbeafe; color:#1d4ed8; }}
.badge.status-draft {{ background:#fef3c7; color:#d97706; }}
.badge.status-eff {{ background:#dcfce7; color:#16a34a; }}
.keyword {{ display:inline-block; border:1px solid #e2e8f0; font-size:11px; padding:2px 8px; border-radius:8px; margin-right:6px; margin-top:6px; }}
.meta {{ color:#64748b; font-size:12px; margin-top:6px; }}
</style>
""", unsafe_allow_html=True)

# ---------------------------
# ì„¤ì •/ìƒìˆ˜
# ---------------------------
USER_AGENT = "Mozilla/5.0 (compatible; RegWatch/1.0)"
HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ko,en;q=0.8"}
MAX_PER_SOURCE = 60

SOURCES = [
    {"id": "echa",  "name": "ECHA (EU â€“ European Chemicals Agency)",      "type": "html", "url": "https://echa.europa.eu/legislation"},
    {"id": "cbp",   "name": "U.S. Customs and Border Protection (CBP)",    "type": "rss",  "url": "https://www.cbp.gov/rss/trade"},
    {"id": "cbppr", "name": "CBP Press Releases",                          "type": "rss",  "url": "https://www.cbp.gov/rss/press-releases"},
    {"id": "motie", "name": "MOTIE (ëŒ€í•œë¯¼êµ­ ì‚°ì—…í†µìƒìì›ë¶€)",                 "type": "html", "url": "https://www.motie.go.kr"},
    {"id": "bmuv",  "name": "BMUV (ë…ì¼ í™˜ê²½ë¶€)",                           "type": "rss",  "url": "https://www.bundesumweltministerium.de/meldungen.rss"}
]

# Potens ìš”ì•½ API (ì„ íƒ)
POTENS_API_KEY = st.secrets.get("POTENS_API_KEY", os.environ.get("POTENS_API_KEY", ""))
POTENS_BASE = os.environ.get("POTENS_BASE", "https://api.potens.ai")
POTENS_SUMMARY_PATH = os.environ.get("POTENS_SUMMARY_PATH", "/v1/summarize")

# ---------------------------
# ìœ í‹¸
# ---------------------------
def md5_hex(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()

def to_iso(s: str) -> str:
    if not s:
        return datetime.now(timezone.utc).isoformat()
    try:
        d = dtparse.parse(s)
        if d.tzinfo is None:
            d = d.replace(tzinfo=timezone.utc)
        return d.astimezone(timezone.utc).isoformat()
    except Exception:
        return datetime.now(timezone.utc).isoformat()

def days_new(iso: str, days=7) -> bool:
    try:
        d = dtparse.parse(iso)
        if d.tzinfo is None:
            d = d.replace(tzinfo=timezone.utc)
        return (datetime.now(timezone.utc) - d) <= timedelta(days=days)
    except Exception:
        return False

def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def simple_summary(text: str) -> str:
    t = clean_text(text)
    if len(t) <= 140: return t
    m = re.match(r"^(.{40,280}?[\.!\?])\s+(.{0,280}?[\.!\?])?", t)
    return (m.group(1) + (" " + m.group(2) if m and m.group(2) else "")) if m else t[:280]

def extract_keywords(text: str, topn=5) -> List[str]:
    stop = set(["the","and","for","with","from","that","this","are","was","were","will","have","has","been","on","of","in","to","a","an","by","ë°","ê³¼","ì—","ì˜","ìœ¼ë¡œ"])
    words = re.sub(r"[^a-z0-9ê°€-í£ ]"," ", (text or "").lower()).split()
    freq={}
    for w in words:
        if len(w)<=2 or w in stop: continue
        freq[w]=freq.get(w,0)+1
    return [w for w,_ in sorted(freq.items(), key=lambda x: x[1], reverse=True)[:topn]]

def guess_category(title: str) -> str:
    t=(title or "").lower()
    if re.search(r"(reach|clp|pfas|biocide|chemical|substance|restriction|authorisation)", t): return "í™”í•™ë¬¼ì§ˆê·œì œ"
    if re.search(r"(tariff|duty|quota|fees|rate|ace|import|export|cbp|customs)", t): return "ë¬´ì—­ì •ì±…"
    if re.search(r"(environment|climate|emission|umwelt|í™˜ê²½)", t): return "í™˜ê²½ê·œì œ"
    if re.search(r"(policy|industry|manufactur|ì‚°ì—…|í˜ì‹ |íˆ¬ì)", t): return "ì‚°ì—…ì •ì±…"
    return "ì‚°ì—…ì •ì±…"

def guess_impact(text: str) -> str:
    t=(text or "").lower()
    if re.search(r"(effective|mandatory|ban|prohibit|enforce|in force|penalty|ì‹œí–‰|ë°œíš¨)", t): return "High"
    if re.search(r"(proposal|draft|consultation|comment|plan|roadmap|ì´ˆì•ˆ|ì˜ê²¬ìˆ˜ë ´)", t): return "Medium"
    return "Low"

def potens_summarize(text: str, language="ko", max_sentences=3) -> str:
    """í¬í…ìŠ¤ ìš”ì•½ API (ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜)"""
    if not POTENS_API_KEY:
        return ""
    try:
        url = POTENS_BASE + POTENS_SUMMARY_PATH
        r = requests.post(url, json={
            "text": text, "language": language, "max_sentences": max_sentences
        }, headers={
            "Authorization": f"Bearer {POTENS_API_KEY}",
            "X-API-Key": POTENS_API_KEY,
            "Content-Type": "application/json",
            "User-Agent": USER_AGENT
        }, timeout=15)
        if r.status_code >= 400:
            return ""
        data = r.json()
        return (data.get("summary") or data.get("result") or data.get("text") or "").strip()
    except Exception:
        return ""

def normalize(source_id, source_name, title, url, date_iso, summary, country_hint="") -> Dict:
    if not url: return {}
    title = clean_text(title or url)
    date_iso = to_iso(date_iso or "")
    category = guess_category(title)
    impact = guess_impact(title + " " + (summary or ""))
    return {
        "id": md5_hex(url),
        "sourceId": source_id,
        "sourceName": source_name,
        "title": title,
        "url": url,
        "dateIso": date_iso,
        "category": category,
        "summary": clean_text(summary or ""),
        "impact": impact,
        "country": country_hint or {"cbp":"ë¯¸êµ­","cbppr":"ë¯¸êµ­","bmuv":"ë…ì¼","echa":"EU","motie":"ëŒ€í•œë¯¼êµ­"}.get(source_id,"")
    }

# ---------------------------
# í¬ë¡¤ëŸ¬ë“¤
# ---------------------------
def fetch_rss(source_id, name, feed_url, country=""):
    out=[]
    d = feedparser.parse(feed_url)
    for e in d.entries[:MAX_PER_SOURCE]:
        title = getattr(e,"title",""); link = getattr(e,"link","")
        pub = getattr(e,"published","") or getattr(e,"updated","") or ""
        desc = getattr(e,"summary","") or getattr(e,"description","") or ""
        summ = potens_summarize(f"{title}\n\n{desc}") or simple_summary(f"{title}. {desc}")
        item = normalize(source_id, name, title, link, pub, summ, country)
        if item: out.append(item)
    return out

def fetch_echa_legislation():
    """ECHA: legislation í˜ì´ì§€ ìš°ì„  â†’ JSON-LD/ë§í¬ â†’ ì‹¤íŒ¨ ì‹œ news í˜ì´ì§€ í´ë°±"""
    items=[]
    try:
        r = requests.get("https://echa.europa.eu/legislation", headers=HEADERS, timeout=15)
        r.raise_for_status()
        html = r.text
        soup = BeautifulSoup(html, "html.parser")

        # 1) JSON-LDì—ì„œ ë¬¸ì„œ/ë‰´ìŠ¤ ìºì¹˜
        for script in soup.find_all("script", {"type":"application/ld+json"}):
            try:
                data = script.string
                if not data: continue
                j = st.session_state.get("_tmpjson", None)
                j = None
                j = __import__("json").loads(data)
                arr = j if isinstance(j,list) else [j]
                for n in arr:
                    title = (n.get("headline") or n.get("name") or "").strip()
                    url   = (n.get("url") or (n.get("mainEntityOfPage") or {}).get("@id") or "").strip()
                    if not title or not url: continue
                    date  = n.get("datePublished") or n.get("dateModified") or ""
                    desc  = n.get("description") or ""
                    summ  = potens_summarize(f"{title}\n\n{desc}") or simple_summary(f"{title}. {desc}")
                    items.append(normalize("echa","ECHA (EU â€“ European Chemicals Agency)", title, url, date, summ, "EU"))
            except Exception:
                continue

        # 2) ë§í¬ ê¸°ë°˜ ë³´ì¡°: /legislation/ ë˜ëŠ” /news í¬í•¨
        if len(items) < 10:
            for a in soup.find_all("a", href=True):
                href = a["href"]
                if not re.search(r"/(legislation|news)", href): continue
                href = href if href.startswith("http") else f"https://echa.europa.eu{href}"
                title = clean_text(a.get_text())
                if not title: continue
                summ = potens_summarize(title) or simple_summary(title)
                items.append(normalize("echa","ECHA (EU â€“ European Chemicals Agency)", title, href, "", summ, "EU"))
                if len(items) >= MAX_PER_SOURCE: break
    except Exception:
        pass

    # 3) í´ë°±: /news í˜ì´ì§€
    if len(items) < 5:
        try:
            r = requests.get("https://echa.europa.eu/news", headers=HEADERS, timeout=15)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")
            for a in soup.select('a[href*="/news"]'):
                href = a.get("href","")
                if not href: continue
                href = href if href.startswith("http") else f"https://echa.europa.eu{href}"
                title = clean_text(a.get_text())
                if not title: continue
                summ = potens_summarize(title) or simple_summary(title)
                items.append(normalize("echa","ECHA (EU â€“ European Chemicals Agency)", title, href, "", summ, "EU"))
                if len(items) >= MAX_PER_SOURCE: break
        except Exception:
            pass

    # dedup by url
    uniq={}
    for it in items: uniq[it["url"]] = it
    return list(uniq.values())[:MAX_PER_SOURCE]

def fetch_motie_generic():
    """
    MOTIE: ê³µì‹ RSSê°€ ì—†ì–´ ê¸°ë³¸ í˜ì´ì§€ì™€ ê³µì§€/ë³´ë„ ìë£Œë¡œ ì¶”ì •ë˜ëŠ” ë§í¬ë¥¼ ê¸ì–´ì˜´
    - ë™ì‘ì´ ì•½í•  ìˆ˜ ìˆì–´ í¬í…ìŠ¤ ìš”ì•½ìœ¼ë¡œ ë³´ì™„
    """
    items=[]
    candidates = [
        "https://www.motie.go.kr",  # í™ˆ(í‘œ/ë¦¬ìŠ¤íŠ¸ì—ì„œ aíƒœê·¸ ê¸ê¸°)
        # í•„ìš”ì‹œ ë³´ë„ìë£Œ/ê³µì§€ ë©”ë‰´ ê²½ë¡œë¥¼ ì¶”ê°€ë¡œ ëŠ˜ë¦´ ìˆ˜ ìˆìŒ
    ]
    for url in candidates:
        try:
            r = requests.get(url, headers=HEADERS, timeout=15)
            r.raise_for_status()
            soup = BeautifulSoup(r.text, "html.parser")
            # í‘œ(tr) ìš°ì„ 
            for tr in soup.find_all("tr"):
                a = tr.find("a", href=True)
                if not a: continue
                href=a["href"]
                if not href.startswith("http"): href = "https://www.motie.go.kr" + href
                title = clean_text(a.get_text())
                if not title: continue
                # ë‚ ì§œ íŒ¨í„´(ìˆì„ ìˆ˜ë„/ì—†ì„ ìˆ˜ë„ ìˆìŒ)
                txt = tr.get_text(" ", strip=True)
                m = re.search(r"(\d{4}[.-]\d{2}[.-]\d{2})", txt)
                date = m.group(1).replace(".", "-") if m else ""
                summ = potens_summarize(title) or simple_summary(title)
                items.append(normalize("motie","MOTIE (ëŒ€í•œë¯¼êµ­ ì‚°ì—…í†µìƒìì›ë¶€)", title, href, date, summ, "ëŒ€í•œë¯¼êµ­"))
                if len(items) >= MAX_PER_SOURCE: break
            if len(items) >= MAX_PER_SOURCE: break

            # ë³´ì¡°: ì¼ë°˜ ë§í¬ ì¤‘ /bbs, /board, /news ë‹¨ì–´ í¬í•¨
            for a in soup.find_all("a", href=True):
                href=a["href"]; text=clean_text(a.get_text())
                if not text or not href: continue
                if not re.search(r"/(bbs|board|news|notice|press)", href, re.I): continue
                if not href.startswith("http"): href = "https://www.motie.go.kr" + href
                summ = potens_summarize(text) or simple_summary(text)
                items.append(normalize("motie","MOTIE (ëŒ€í•œë¯¼êµ­ ì‚°ì—…í†µìƒìì›ë¶€)", text, href, "", summ, "ëŒ€í•œë¯¼êµ­"))
                if len(items) >= MAX_PER_SOURCE: break
        except Exception:
            continue

    uniq={}
    for it in items: uniq[it["url"]] = it
    return list(uniq.values())[:MAX_PER_SOURCE]

# ---------------------------
# ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
# ---------------------------
def fetch_all(selected_ids: List[str]) -> List[Dict]:
    out=[]
    for s in SOURCES:
        if s["id"] not in selected_ids: continue
        try:
            if s["type"] == "rss":
                out += fetch_rss(s["id"], s["name"], s["url"])
            elif s["id"] == "echa":
                out += fetch_echa_legislation()
            elif s["id"] == "motie":
                out += fetch_motie_generic()
        except Exception:
            continue
    # dedup by url
    uniq={}
    for it in out: uniq[it["url"]] = it
    return list(uniq.values())

# ---------------------------
# ë³´ê³ ì„œ ìƒì„±/ë‹¤ìš´ë¡œë“œ
# ---------------------------
def to_dataframe(items: List[Dict]) -> pd.DataFrame:
    if not items: return pd.DataFrame(columns=["date","title","agency","country","category","impact","url"])
    rows=[]
    for d in items:
        rows.append([d["dateIso"], d["title"], d["sourceName"], d.get("country",""), d["category"], d["impact"], d["url"]])
    df = pd.DataFrame(rows, columns=["date","title","agency","country","category","impact","url"])
    # ìµœì‹ ìˆœ
    try:
        df["date_dt"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.sort_values("date_dt", ascending=False).drop(columns=["date_dt"])
    except Exception:
        pass
    return df

def make_markdown_report(df: pd.DataFrame, since_days: int) -> str:
    if df.empty:
        return "# ê·œì œ/ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ë³´ê³ ì„œ\n\në°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    head = f"# ê·œì œ/ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ë³´ê³ ì„œ\n\n- ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n- ê¸°ì¤€: ìµœê·¼ {since_days}ì¼\n- ì´ í•­ëª©: {len(df)}ê±´\n"
    # ì¹´í…Œê³ ë¦¬/ê¸°ê´€ ìš”ì•½
    summary_cat = df["category"].value_counts().to_dict()
    summary_ag  = df["agency"].value_counts().to_dict()
    lines = [head, "## ìš”ì•½(ì¹´í…Œê³ ë¦¬)",]
    for k,v in summary_cat.items(): lines.append(f"- {k}: {v}ê±´")
    lines.append("\n## ìš”ì•½(ê¸°ê´€)")
    for k,v in summary_ag.items(): lines.append(f"- {k}: {v}ê±´")
    lines.append("\n## ìƒì„¸ ëª©ë¡")
    for _,r in df.iterrows():
        lines.append(f"- **[{r['title']}]({r['url']})**  \n  - ê¸°ê´€: {r['agency']} | êµ­ê°€: {r['country']} | ë¶„ë¥˜: {r['category']} | ì˜í–¥ë„: {r['impact']}  \n  - ë‚ ì§œ: {r['date']}")
    return "\n".join(lines)

def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    return buf.getvalue().encode("utf-8-sig")

# ---------------------------
# UI ë³¸ë¬¸
# ---------------------------
st.markdown(f"""
<div class="big-header">
  <span class="brand-title">RegWatch</span>
  <span class="subtitle">ê¸€ë¡œë²Œ ê·œì œ ëª¨ë‹ˆí„°ë§(ë³´ê³ ì„œ)</span>
</div>
""", unsafe_allow_html=True)

left, mid, right = st.columns([2,2,3])
with left:
    sel = st.multiselect("ìˆ˜ì§‘ ëŒ€ìƒ", [s["id"] for s in SOURCES],
                         default=[s["id"] for s in SOURCES],
                         format_func=lambda sid: next(s["name"] for s in SOURCES if s["id"]==sid))
with mid:
    since_days = st.slider("ìµœê·¼ Nì¼ë§Œ ë³´ê¸°", 3, 60, 14)
with right:
    do = st.button("ì—…ë°ì´íŠ¸ ì‹¤í–‰", use_container_width=True, help="ì„ íƒí•œ ì‚¬ì´íŠ¸ì—ì„œ ìµœì‹  í•­ëª©ì„ ìˆ˜ì§‘í•˜ê³  ìš”ì•½í•©ë‹ˆë‹¤.")

if do or "items" not in st.session_state:
    with st.spinner("ìˆ˜ì§‘Â·ìš”ì•½ ì¤‘..."):
        items = fetch_all(sel or [s["id"] for s in SOURCES])
        # í¬í…ìŠ¤ í‚¤ê°€ ì—†ìœ¼ë©´ simple_summaryê°€ ì´ë¯¸ ì ìš©ë¨. (RSSì˜ desc ë˜ëŠ” ì œëª© ìœ„ì£¼)
        st.session_state.items = items

items = st.session_state.get("items", [])

# ê¸°ê°„ í•„í„°
cut = datetime.now(timezone.utc) - timedelta(days=since_days)
def _in_range(x):
    try:
        return dtparse.parse(x) >= cut
    except Exception:
        return True
items_recent = [d for d in items if _in_range(d["dateIso"])]

# í‘œ/ì¹´ë“œ/ë³´ê³ ì„œ
df = to_dataframe(items_recent)
st.subheader(f"ì´ {len(df)}ê±´ Â· ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

# 1) í‘œ
st.dataframe(df, use_container_width=True, hide_index=True)

# 2) ì¹´ë“œ ë·°
st.markdown("---")
st.subheader("ì¹´ë“œ ë³´ê¸°")
for d in items_recent:
    is_new = days_new(d["dateIso"], 7)
    kw = extract_keywords(d["title"] + " " + d["summary"])
    cat_class = {"í™”í•™ë¬¼ì§ˆê·œì œ":"cat-chem","ë¬´ì—­ì •ì±…":"cat-trade","ì‚°ì—…ì •ì±…":"cat-ind","í™˜ê²½ê·œì œ":"cat-env"}.get(d["category"],"")
    status = "EFFECTIVE" if d["impact"]=="High" else ("DRAFT" if d["impact"]=="Medium" else "ANNOUNCED")
    status_class = {"ANNOUNCED":"badge status-ann","DRAFT":"badge status-draft","EFFECTIVE":"badge status-eff"}[status]
    st.markdown(f"<div class='card {'new' if is_new else ''}'>", unsafe_allow_html=True)
    st.markdown(f"<span class='badge {cat_class}'>{d['category']}</span> "
                f"<span class='badge kor'>{d.get('country','')}</span> "
                f"<span class='{status_class}'>{status}</span>", unsafe_allow_html=True)
    st.markdown(f"<h4>{d['title']}</h4>", unsafe_allow_html=True)
    st.write(d["summary"] or "")
    st.markdown(f"<div class='meta'><b>ê¸°ê´€</b>Â·{d['sourceName']} | <b>ë‚ ì§œ</b>Â·{d['dateIso']}</div>", unsafe_allow_html=True)
    if kw:
        st.markdown(" ".join([f"<span class='keyword'>{k}</span>" for k in kw]), unsafe_allow_html=True)
    st.markdown(f"[ì›ë¬¸ ë³´ê¸°]({d['url']})")
    st.markdown("</div>", unsafe_allow_html=True)

# 3) ë³´ê³ ì„œ(Markdown) + ë‹¤ìš´ë¡œë“œ
st.markdown("---")
st.subheader("ë³´ê³ ì„œ ìƒì„±")
md = make_markdown_report(df, since_days)
st.download_button("ğŸ“„ Markdown ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ", data=md.encode("utf-8"),
                   file_name=f"regwatch_report_{datetime.now().strftime('%Y%m%d_%H%M')}.md",
                   mime="text/markdown")
st.download_button("ğŸ§¾ CSV ë‹¤ìš´ë¡œë“œ", data=df_to_csv_bytes(df),
                   file_name=f"regwatch_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                   mime="text/csv")
