# -*- coding: utf-8 -*-
"""
RegWatch (Streamlit / No external AI API)
- ëŒ€ìƒ ì‚¬ì´íŠ¸:
  â€¢ ECHA: https://echa.europa.eu/legislation  (+ news í´ë°±)
  â€¢ CBP: https://www.cbp.gov/  (RSS 2ì¢… - trade, press)
  â€¢ MOTIE: https://www.motie.go.kr/
  â€¢ BMUV: https://www.bundesumweltministerium.de/ (RSS)
- í•µì‹¬ ìˆ˜ì •: RSSë¥¼ requestsë¡œ ë°›ì•„ì„œ feedparser.parse(bytes) ì²˜ë¦¬ (User-Agent ì§€ì •)
- ë””ë²„ê·¸ ë¡œê·¸/ìºì‹œì´ˆê¸°í™” ë²„íŠ¼ ì¶”ê°€
"""

import re, os, io, hashlib, json
from datetime import datetime, timezone, timedelta
from typing import List, Dict

import requests, feedparser, pandas as pd
from bs4 import BeautifulSoup
from dateutil import parser as dtparse
import streamlit as st

# ---------------------------
# UI ê¸°ë³¸ ì„¤ì •
# ---------------------------
st.set_page_config(page_title="RegWatch â€“ ê¸€ë¡œë²Œ ê·œì œ ëª¨ë‹ˆí„°ë§", layout="wide")
BRAND = "#0f2e69"; ACCENT = "#dc8d32"
st.markdown(f"""
<style>
@import url('https://cdn.jsdelivr.net/gh/orioncactus/pretendard@v1.3.8/dist/web/static/pretendard.css');
html, body, [class*="css"] {{ font-family: 'Pretendard', -apple-system, BlinkMacSystemFont, system-ui, Roboto, sans-serif; }}
.big-header {{ background: linear-gradient(135deg, {BRAND} 0%, #1a4b8c 100%); color:#fff; padding:18px 22px; border-radius:10px; margin-bottom:12px; }}
.brand-title {{ font-weight:800; font-size:22px; margin-right:10px; }}
.card {{ border:1px solid #e2e8f0; border-left:5px solid transparent; border-radius:12px; padding:16px; margin:12px 0; background:#fff; }}
.card.new {{ border-left-color:{ACCENT}; }}
.card h4 {{ color:{BRAND}; margin:0 0 8px 0; font-size:18px; }}
.badge {{ display:inline-block; padding:3px 8px; border-radius:999px; font-size:11px; font-weight:700; margin-right:6px; }}
.badge.kor {{ background:#f1f5f9; color:#475569; }}
.badge.cat-chem {{ background:#e0f2fe; color:#0277bd; }}
.badge.cat-trade {{ background:#f3e5f5; color:#7b1fa2; }}
.badge.cat-ind {{ background:#e8f5e8; color:#2e7d32; }}
.badge.cat-env {{ background:#fff3e0; color:#f57c00; }}
.badge.status-ann {{ background:#dbeafe; color:#1d4ed8; }}
.badge.status-draft {{ background:#fef3c7; color:#d97706; }}
.badge.status-eff {{ background:#dcfce7; color:#16a34a; }}
.keyword {{ display:inline-block; border:1px solid #e2e8f0; font-size:11px; padding:2px 8px; border-radius:8px; margin-right:6px; margin-top:6px; }}
.meta {{ color:#64748b; font-size:12px; margin-top:6px; }}
hr.sep {{ border:none; border-top:1px solid #e2e8f0; margin:16px 0; }}
small.mono {{ font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace; }}
</style>
""", unsafe_allow_html=True)

# ---------------------------
# ìƒìˆ˜/ì„¤ì •
# ---------------------------
USER_AGENT = "Mozilla/5.0 (compatible; RegWatch/1.0; +https://streamlit.io)"
HEADERS = {"User-Agent": USER_AGENT, "Accept-Language": "ko,en;q=0.8"}
TIMEOUT = 20
MAX_PER_SOURCE = 60

SOURCES = [
    {"id": "echa",  "name": "ECHA (EU â€“ European Chemicals Agency)", "type": "html", "url": "https://echa.europa.eu/legislation"},
    {"id": "cbp",   "name": "U.S. Customs and Border Protection (CBP)", "type": "rss-multi",
     "urls": ["https://www.cbp.gov/rss/trade", "https://www.cbp.gov/rss/press-releases"]},
    {"id": "motie", "name": "MOTIE (ëŒ€í•œë¯¼êµ­ ì‚°ì—…í†µìƒìì›ë¶€)", "type": "html", "url": "https://www.motie.go.kr/"},
    {"id": "bmuv",  "name": "BMUV (ë…ì¼ í™˜ê²½ë¶€)", "type": "rss", "url": "https://www.bundesumweltministerium.de/meldungen.rss"},
]

# ---------------------------
# ë¡œê·¸ ìœ í‹¸
# ---------------------------
if "logs" not in st.session_state:
    st.session_state.logs = []

def log(msg):
    ts = datetime.now().strftime("%H:%M:%S")
    st.session_state.logs.append(f"[{ts}] {msg}")

def clear_logs():
    st.session_state.logs = []

# ---------------------------
# í—¬í¼ë“¤
# ---------------------------
def md5_hex(s: str) -> str:
    import hashlib
    return hashlib.md5(s.encode("utf-8")).hexdigest()

def to_iso(s: str) -> str:
    if not s:
        return datetime.now(timezone.utc).isoformat()
    try:
        d = dtparse.parse(s)
        if d.tzinfo is None:
            d = d.replace(tzinfo=timezone.utc)
        return d.astimezone(timezone.utc).isoformat()
    except Exception:
        return datetime.now(timezone.utc).isoformat()

def days_new(iso: str, days=7) -> bool:
    try:
        d = dtparse.parse(iso)
        if d.tzinfo is None: d = d.replace(tzinfo=timezone.utc)
        return (datetime.now(timezone.utc) - d) <= timedelta(days=days)
    except Exception:
        return False

def clean_text(s: str) -> str:
    return re.sub(r"\s+", " ", (s or "")).strip()

def split_sentences(text: str):
    t = clean_text(text)
    return [p for p in re.split(r'(?<=[\.!\?]|ë‹¤\.|ìš”\.)\s+', t) if p]

def simple_summary(text: str, max_sentences=2, max_len=320) -> str:
    sents = split_sentences(text)
    if not sents:
        return clean_text(text)[:max_len]
    out = " ".join(sents[:max_sentences])
    return out[:max_len]

def extract_keywords(text: str, topn=5):
    stop = set(["the","and","for","with","from","that","this","are","was","were","will","have","has","been","on","of","in","to","a","an","by","ë°","ê³¼","ì—","ì˜","ìœ¼ë¡œ"])
    words = re.sub(r"[^a-z0-9ê°€-í£ ]"," ", (text or "").lower()).split()
    freq={}
    for w in words:
        if len(w)<=2 or w in stop: continue
        freq[w]=freq.get(w,0)+1
    return [w for w,_ in sorted(freq.items(), key=lambda x: x[1], reverse=True)[:topn]]

def guess_category(title: str) -> str:
    t=(title or "").lower()
    if re.search(r"(reach|clp|pfas|biocide|chemical|substance|restriction|authorisation)", t): return "í™”í•™ë¬¼ì§ˆê·œì œ"
    if re.search(r"(tariff|duty|quota|fees|rate|ace|import|export|cbp|customs)", t): return "ë¬´ì—­ì •ì±…"
    if re.search(r"(environment|climate|emission|umwelt|í™˜ê²½)", t): return "í™˜ê²½ê·œì œ"
    if re.search(r"(policy|industry|manufactur|ì‚°ì—…|í˜ì‹ |íˆ¬ì)", t): return "ì‚°ì—…ì •ì±…"
    return "ì‚°ì—…ì •ì±…"

def guess_impact(text: str) -> str:
    t=(text or "").lower()
    if re.search(r"(effective|mandatory|ban|prohibit|enforce|in force|penalty|ì‹œí–‰|ë°œíš¨)", t): return "High"
    if re.search(r"(proposal|draft|consultation|comment|plan|roadmap|ì´ˆì•ˆ|ì˜ê²¬ìˆ˜ë ´)", t): return "Medium"
    return "Low"

def country_of(source_id: str) -> str:
    return {"cbp":"ë¯¸êµ­","bmuv":"ë…ì¼","motie":"ëŒ€í•œë¯¼êµ­","echa":"EU"}.get(source_id,"")

def normalize(source_id, source_name, title, url, date_iso, summary) -> Dict:
    if not url: return {}
    title = clean_text(title or url)
    date_iso = to_iso(date_iso or "")
    category = guess_category(title)
    impact = guess_impact(title + " " + (summary or ""))
    return {
        "id": md5_hex(url),
        "sourceId": source_id,
        "sourceName": source_name,
        "title": title,
        "url": url,
        "dateIso": date_iso,
        "category": category,
        "summary": clean_text(summary or ""),
        "impact": impact,
        "country": country_of(source_id),
    }

# ---------------------------
# HTTP
# ---------------------------
@st.cache_data(ttl=1800, show_spinner=False)
def http_get(url: str) -> str:
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    return r.text

def http_get_bytes(url: str) -> bytes:
    r = requests.get(url, headers=HEADERS, timeout=TIMEOUT)
    r.raise_for_status()
    return r.content

# ---------------------------
# ìˆ˜ì§‘ê¸°
# ---------------------------
def fetch_rss_one(source_id, name, feed_url) -> List[Dict]:
    """RSSë¥¼ requestsë¡œ ë°›ì•„ feedparser.parse(bytes)ë¡œ íŒŒì‹±(í—¤ë” ë³´ì¥)"""
    out=[]
    try:
        raw = http_get_bytes(feed_url)
        d = feedparser.parse(raw)
        n = 0
        for e in d.entries[:MAX_PER_SOURCE]:
            title = getattr(e,"title",""); link = getattr(e,"link","")
            pub   = getattr(e,"published","") or getattr(e,"updated","") or ""
            desc  = getattr(e,"summary","") or getattr(e,"description","") or ""
            summary = simple_summary(f"{title}. {desc}")
            it = normalize(source_id, name, title, link, pub, summary)
            if it: out.append(it); n += 1
        log(f"RSS OK [{name}] {n}ê±´")
    except Exception as ex:
        log(f"RSS FAIL [{name}] {type(ex).__name__}: {ex}")
    return out

def fetch_rss_multi(source_id, name, feed_urls: List[str]) -> List[Dict]:
    out=[]
    for u in feed_urls:
        out += fetch_rss_one(source_id, name, u)
    # dedup by url
    uniq={}
    for it in out: uniq[it["url"]] = it
    return list(uniq.values())[:MAX_PER_SOURCE]

def fetch_echa_legislation() -> List[Dict]:
    items=[]
    try:
        html = http_get("https://echa.europa.eu/legislation")
        soup = BeautifulSoup(html, "html.parser")

        # JSON-LD
        count = 0
        for s in soup.find_all("script", {"type":"application/ld+json"}):
            try:
                j = json.loads(s.string or "")
                arr = j if isinstance(j, list) else [j]
                for n in arr:
                    title = (n.get("headline") or n.get("name") or "").strip()
                    url   = (n.get("url") or (n.get("mainEntityOfPage") or {}).get("@id") or "").strip()
                    if not title or not url: continue
                    date  = n.get("datePublished") or n.get("dateModified") or ""
                    desc  = n.get("description") or ""
                    items.append(normalize("echa","ECHA (EU â€“ European Chemicals Agency)", title, url, date, simple_summary(desc or title)))
                    count += 1
            except Exception:
                continue
        log(f"ECHA JSON-LD ì¶”ì¶œ {count}ê±´")

        # ë§í¬ ë³´ì¡°
        if len(items) < 10:
            added = 0
            for a in soup.find_all("a", href=True):
                href = a["href"]; text = a.get_text(strip=True)
                if not href or not text: continue
                if not re.search(r"/(legislation|news)", href): continue
                href = href if href.startswith("http") else f"https://echa.europa.eu{href}"
                items.append(normalize("echa","ECHA (EU â€“ European Chemicals Agency)", text, href, "", simple_summary(text)))
                added += 1
                if len(items) >= MAX_PER_SOURCE: break
            log(f"ECHA ë§í¬ ë³´ì¡° {added}ê±´")
    except Exception as ex:
        log(f"ECHA FAIL: {type(ex).__name__}: {ex}")

    # news í´ë°±
    if len(items) < 5:
        try:
            html = http_get("https://echa.europa.eu/news")
            soup = BeautifulSoup(html, "html.parser")
            added = 0
            for a in soup.select('a[href*="/news"]'):
                href = a.get("href","")
                if not href: continue
                href = href if href.startswith("http") else f"https://echa.europa.eu{href}"
                title = a.get_text(strip=True)
                if not title: continue
                items.append(normalize("echa","ECHA (EU â€“ European Chemicals Agency)", title, href, "", simple_summary(title)))
                added += 1
                if len(items) >= MAX_PER_SOURCE: break
            log(f"ECHA ë‰´ìŠ¤ í´ë°± {added}ê±´")
        except Exception as ex:
            log(f"ECHA NEWS FAIL: {type(ex).__name__}: {ex}")

    uniq={}
    for it in items: uniq[it["url"]] = it
    return list(uniq.values())[:MAX_PER_SOURCE]

def fetch_motie_generic() -> List[Dict]:
    """MOTIE: í‘œ(tr) + /bbs|board|news|notice|press ë§í¬ íœ´ë¦¬ìŠ¤í‹±"""
    items=[]
    try:
        html = http_get("https://www.motie.go.kr/")
        soup = BeautifulSoup(html, "html.parser")

        added1 = 0
        for tr in soup.find_all("tr"):
            a = tr.find("a", href=True)
            if not a: continue
            href = a["href"]; title = a.get_text(strip=True)
            if not title: continue
            if not href.startswith("http"): href = "https://www.motie.go.kr" + href
            txt = tr.get_text(" ", strip=True)
            m = re.search(r"(\d{4}[.\-]\d{2}[.\-]\d{2})", txt)
            date = (m.group(1).replace(".", "-") if m else "")
            items.append(normalize("motie","MOTIE (ëŒ€í•œë¯¼êµ­ ì‚°ì—…í†µìƒìì›ë¶€)", title, href, date, simple_summary(title)))
            added1 += 1
            if len(items) >= MAX_PER_SOURCE: break

        added2 = 0
        if len(items) < 10:
            for a in soup.find_all("a", href=True):
                href=a["href"]; text=a.get_text(strip=True)
                if not href or not text: continue
                if not re.search(r"/(bbs|board|news|notice|press)", href, re.I): continue
                if not href.startswith("http"): href = "https://www.motie.go.kr" + href
                items.append(normalize("motie","MOTIE (ëŒ€í•œë¯¼êµ­ ì‚°ì—…í†µìƒìì›ë¶€)", text, href, "", simple_summary(text)))
                added2 += 1
                if len(items) >= MAX_PER_SOURCE: break
        log(f"MOTIE ì¶”ì¶œ tr:{added1}ê±´ + link:{added2}ê±´")
    except Exception as ex:
        log(f"MOTIE FAIL: {type(ex).__name__}: {ex}")

    uniq={}
    for it in items: uniq[it["url"]] = it
    return list(uniq.values())[:MAX_PER_SOURCE]

# ---------------------------
# ì „ì²´ ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
# ---------------------------
def fetch_all(selected_ids: List[str]) -> List[Dict]:
    out=[]
    for s in SOURCES:
        if s["id"] not in selected_ids: continue
        try:
            if s["type"]=="rss":
                out += fetch_rss_one(s["id"], s["name"], s["url"])
            elif s["type"]=="rss-multi":
                out += fetch_rss_multi(s["id"], s["name"], s["urls"])
            elif s["id"]=="echa":
                out += fetch_echa_legislation()
            elif s["id"]=="motie":
                out += fetch_motie_generic()
        except Exception as ex:
            log(f"PIPE FAIL [{s['name']}]: {type(ex).__name__}: {ex}")
            continue
    # dedup
    uniq={}
    for it in out: uniq[it["url"]] = it
    log(f"ì´ ìˆ˜ì§‘ {len(uniq)}ê±´")
    return list(uniq.values())

# ---------------------------
# ë³´ê³ ì„œ/ë‹¤ìš´ë¡œë“œ
# ---------------------------
def to_dataframe(items: List[Dict]) -> pd.DataFrame:
    if not items:
        return pd.DataFrame(columns=["date","title","agency","country","category","impact","url"])
    rows=[]
    for d in items:
        rows.append([d["dateIso"], d["title"], d["sourceName"], d.get("country",""), d["category"], d["impact"], d["url"]])
    df = pd.DataFrame(rows, columns=["date","title","agency","country","category","impact","url"])
    try:
        df["date_dt"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.sort_values("date_dt", ascending=False).drop(columns=["date_dt"])
    except Exception:
        pass
    return df

def make_markdown_report(df: pd.DataFrame, since_days: int) -> str:
    if df.empty:
        return "# ê·œì œ/ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ë³´ê³ ì„œ\n\në°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    head = f"# ê·œì œ/ë‰´ìŠ¤ ì—…ë°ì´íŠ¸ ë³´ê³ ì„œ\n\n- ìƒì„± ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n- ê¸°ì¤€: ìµœê·¼ {since_days}ì¼\n- ì´ í•­ëª©: {len(df)}ê±´\n"
    summary_cat = df["category"].value_counts().to_dict()
    summary_ag  = df["agency"].value_counts().to_dict()
    lines = [head, "## ìš”ì•½(ì¹´í…Œê³ ë¦¬)"]
    for k,v in summary_cat.items(): lines.append(f"- {k}: {v}ê±´")
    lines.append("\n## ìš”ì•½(ê¸°ê´€)")
    for k,v in summary_ag.items(): lines.append(f"- {k}: {v}ê±´")
    lines.append("\n## ìƒì„¸ ëª©ë¡")
    for _,r in df.iterrows():
        lines.append(f"- **[{r['title']}]({r['url']})**  \n  - ê¸°ê´€: {r['agency']} | êµ­ê°€: {r['country']} | ë¶„ë¥˜: {r['category']} | ì˜í–¥ë„: {r['impact']}  \n  - ë‚ ì§œ: {r['date']}")
    return "\n".join(lines)

def df_to_csv_bytes(df: pd.DataFrame) -> bytes:
    buf = io.StringIO()
    df.to_csv(buf, index=False)
    return buf.getvalue().encode("utf-8-sig")

# ---------------------------
# UI ë³¸ë¬¸
# ---------------------------
st.markdown(f"""
<div class="big-header">
  <span class="brand-title">RegWatch</span>
  <span class="subtitle">ê¸€ë¡œë²Œ ê·œì œ ëª¨ë‹ˆí„°ë§ (API ì—†ì´ ê°„ì´ìš”ì•½)</span>
</div>
""", unsafe_allow_html=True)

top1, top2, top3 = st.columns([2,2,2])
with top1:
    selected = st.multiselect(
        "ìˆ˜ì§‘ ëŒ€ìƒ",
        [s["id"] for s in SOURCES],
        default=[s["id"] for s in SOURCES],
        format_func=lambda sid: next(s["name"] for s in SOURCES if s["id"]==sid)
    )
with top2:
    since_days = st.slider("ìµœê·¼ Nì¼ë§Œ ë³´ê¸°", 3, 60, 14)
with top3:
    colA, colB = st.columns([1,1])
    with colA:
        do = st.button("ì—…ë°ì´íŠ¸ ì‹¤í–‰", use_container_width=True)
    with colB:
        if st.button("ìºì‹œ ì´ˆê¸°í™”", use_container_width=True):
            st.cache_data.clear()
            clear_logs()
            st.success("HTTP ìºì‹œë¥¼ ë¹„ì› ìŠµë‹ˆë‹¤.")

if do or "items" not in st.session_state:
    clear_logs()
    with st.spinner("ìˆ˜ì§‘Â·ìš”ì•½ ì¤‘..."):
        st.session_state.items = fetch_all(selected or [s["id"] for s in SOURCES])

items = st.session_state.get("items", [])

# ê¸°ê°„ í•„í„°
cut = datetime.now(timezone.utc) - timedelta(days=since_days)
def in_range(iso):
    try: return dtparse.parse(iso) >= cut
    except Exception: return True
items_recent = [d for d in items if in_range(d["dateIso"])]

# ê²€ìƒ‰/ì¹´í…Œê³ ë¦¬ í•„í„°
st.markdown("<hr class='sep'/>", unsafe_allow_html=True)
f1, f2 = st.columns([2,2])
with f1:
    q = st.text_input("ê²€ìƒ‰ì–´(ì œëª©/ìš”ì•½/ê¸°ê´€/êµ­ê°€)")
with f2:
    cat = st.selectbox("ì¹´í…Œê³ ë¦¬", ["ì „ì²´","í™”í•™ë¬¼ì§ˆê·œì œ","ë¬´ì—­ì •ì±…","ì‚°ì—…ì •ì±…","í™˜ê²½ê·œì œ"], index=0)

data = items_recent
if q:
    ql = q.lower()
    data = [d for d in data if ql in (d["title"]+" "+d["summary"]+" "+d["sourceName"]+" "+d.get("country","")).lower()]
if cat != "ì „ì²´":
    data = [d for d in data if d["category"] == cat]

# í‘œ
df = to_dataframe(data)
st.subheader(f"ì´ {len(df)}ê±´ Â· ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
st.dataframe(df, use_container_width=True, hide_index=True)

# ì¹´ë“œ
st.markdown("<hr class='sep'/>", unsafe_allow_html=True)
st.subheader("ì¹´ë“œ ë³´ê¸°")
for d in data:
    is_new = days_new(d["dateIso"], 7)
    kw = extract_keywords(d["title"] + " " + d["summary"])
    cat_class = {"í™”í•™ë¬¼ì§ˆê·œì œ":"cat-chem","ë¬´ì—­ì •ì±…":"cat-trade","ì‚°ì—…ì •ì±…":"cat-ind","í™˜ê²½ê·œì œ":"cat-env"}.get(d["category"],"")
    status = "EFFECTIVE" if d["impact"]=="High" else ("DRAFT" if d["impact"]=="Medium" else "ANNOUNCED")
    status_class = {"ANNOUNCED":"badge status-ann","DRAFT":"badge status-draft","EFFECTIVE":"badge status-eff"}[status]

    st.markdown(f"<div class='card {'new' if is_new else ''}'>", unsafe_allow_html=True)
    st.markdown(f"<span class='badge {cat_class}'>{d['category']}</span> "
                f"<span class='badge kor'>{d.get('country','')}</span> "
                f"<span class='{status_class}'>{status}</span>", unsafe_allow_html=True)
    st.markdown(f"<h4>{d['title']}</h4>", unsafe_allow_html=True)
    st.write(d["summary"] or "")
    st.markdown(f"<div class='meta'><b>ê¸°ê´€</b>Â·{d['sourceName']} | <b>ë‚ ì§œ</b>Â·{d['dateIso']}</div>", unsafe_allow_html=True)
    if kw:
        st.markdown(" ".join([f"<span class='keyword'>{k}</span>" for k in kw]), unsafe_allow_html=True)
    st.markdown(f"[ì›ë¬¸ ë³´ê¸°]({d['url']})")
    st.markdown("</div>", unsafe_allow_html=True)

# ë³´ê³ ì„œ + ë‹¤ìš´ë¡œë“œ
st.markdown("<hr class='sep'/>", unsafe_allow_html=True)
st.subheader("ë³´ê³ ì„œ ìƒì„±")
md = make_markdown_report(df, since_days)
st.download_button("ğŸ“„ Markdown ë³´ê³ ì„œ ë‹¤ìš´ë¡œë“œ", data=md.encode("utf-8"),
                   file_name=f"regwatch_report_{datetime.now().strftime('%Y%m%d_%H%M')}.md",
                   mime="text/markdown")
st.download_button("ğŸ§¾ CSV ë‹¤ìš´ë¡œë“œ", data=df_to_csv_bytes(df),
                   file_name=f"regwatch_{datetime.now().strftime('%Y%m%d_%H%M')}.csv",
                   mime="text/csv")

# ë””ë²„ê·¸ ë¡œê·¸
with st.expander("ë””ë²„ê·¸ ë¡œê·¸ ë³´ê¸°"):
    if not st.session_state.logs:
        st.caption("ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. 'ì—…ë°ì´íŠ¸ ì‹¤í–‰' í›„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        st.markdown("<small class='mono'>" + "<br/>".join(st.session_state.logs) + "</small>", unsafe_allow_html=True)
